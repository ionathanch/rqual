\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[left=1.5in, right=1.5in, top=1.3in, bottom=1.3in]{geometry}
\usepackage{mlmodern}
\usepackage{amsmath,amssymb}
\usepackage[english]{babel}
\usepackage[square, authoryear]{natbib}
\usepackage[colorlinks,citecolor=blue]{hyperref}
\usepackage[capitalize,noabbrev,nameinlink]{cleveref}
\usepackage[margin=2\parindent]{caption}
\usepackage{mathpartir,mathtools,stmaryrd}
\usepackage{enumitem,doi,xspace,minted,tikz-cd}

\title{\textbf{Internalizing Extensions \\ in Lattices of Type Theories}}
\author{Jonathan Chan}
\date{21 November 2024}

\bibliographystyle{ACM-Reference-Format}

\setlength\parskip{1ex plus 0.1em minus 0.2em}

% If needed, define \crefrangeformat, \crefmultiformat, \crefrangemultiformat
\crefformat{question}{#2Question~#1#3}
\Crefformat{question}{#2Question~#1#3}

\newcommand{\note}[1]{\textcolor{red}{[#1]}}
\newcommand{\ie}{\textit{i.e.}\@\xspace}
\newcommand{\eg}{\textit{e.g.}\@\xspace}
\newcommand{\apost}{\textit{a posteriori}\@\xspace}
\newcommand{\kw}[1]{\mathsf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\HH}{\mathsf{H}}
\newcommand{\LL}{\mathsf{L}}
\renewcommand{\SS}{\mathsf{S}}
\newcommand{\oo}{\mathsf{s}}
\newcommand{\ii}{\mathsf{p}}

\begin{document}
\maketitle

\begin{abstract}
  While many proof assistants are founded upon common theoretical ground,
  they also feature further extensions and axioms
  that augment their reasoning power.
  Because some extensions are mutually inconsistent,
  proof assistants support tracking which are used where
  and checking that incompatible extensions are not enabled simultaneously.
  However, this extension tracking is \emph{external} to the type system,
  and it is not possible to refer to the fact that a term uses a specific set of extensions.
  Furthermore, is it not possible refer to a definition that uses an incompatible extension
  even if it is never used in an inconsistent manner.
  If extension tracking could be \emph{internalized},
  then we would have a framework to talk about the properties of different extensions
  within the type system itself.

  A first step towards a framework of extensions could use
  the Dependent Calculus of Indistinguishability (DCOI)~\citep{dcoi}.
  DCOI is a type system with dependency tracking,
  where terms and variables are assigned dependency levels alongside their types.
  These dependency levels form a lattice that describes
  which levels are permitted to access what.
  This report explores how extensions could correspond to dependency levels,
  and how the lattice would describe how extensions are permitted to interact.
\end{abstract}

\section{Introduction}

At the core of a proof assistant founded on the Curry--Howard correspondence
is a type checker that validates a proof of a proposition---a term inhabiting
a corresponding type in a dependent type theory.
These type theories are typically based on some flavour of
Martin-L\"of Type Theory (MLTT)~\citep{mltt}
or the Calculus of Inductive Constructions (CIC)~\citep{cic}.
In practice, a proof assistant doesn't implement merely one type theory,
but a whole host of them, as they include language extensions
that augment or modify its reasoning power.

These type theoretic extensions can consist of additional typing rules,
constructs, and/or definitional equalities.
Because each extension embodies semantically distinct reasoning principles,
enabling an extension results in a separate theory altogether.
There also exist extensions that are mutually incompatible
because together they violate logical consistency.
For instance, \emph{uniqueness of identity proofs},
which propositionally equates all proofs of the same equality,
is incompatible with \emph{univalence},
which adds additional and provably distinct proofs of equality.
In \cref{sec:extensions},
I describe these extensions and a few more found in select proof assistants,
and the ways some combinations are incompatible.

Thus proof assistants are careful to track
the usage of language extensions to rule out inconsistencies.
However, the tracking done by their type checkers is \emph{external} to the type system:
within the language itself,
one cannot assert that a definition is permitted to depend on a particular extension,
nor that it is prohibited from using an extension.
Furthermore, given two incompatible extensions,
a definition in one extension's theory may not be used at all
in the another extension's theory, not even just in a type.
This means one theory cannot be used as a metatheory
to prove properties about definitions in the other theory
if those theories are incompatible.
For instance, considering classical axioms as a language extension,
one would not be able to explore what is constructively proveable
about classical principles.

Ultimately, what is missing is a framework for describing
fine-grained control of proofs and programs across multiple type theories,
where even incompatible theories can interact in interesting ways.
These properties are reminiscent of type systems with \emph{dependency analysis}.
In such systems, terms are stratified by \emph{dependency levels},
which can be thought of intuitively as permission levels tracking permitted usages.
Even if we do not have access to a particular level,
terms at that level can still be manipulated and reasoned about
as long as they are not inspected or evaluated.

I have worked on a dependent type system with dependency tracking,
the Dependent Calculus of Indistinguishability (DCOI)~\citep{dcoi},
along with its variant DCOI$^\omega$~\citep{dcoi-omega},
which is a logically consistent type theory.
DCOI could potentially be used as a basis for a framework
which internalizes extension tracking by stratifying
their corresponding type theories into hierarchies of dependency levels,
where compatibility between extensions maps to the intuitive notion of permission.
I describe the key properties of DCOI relevant for this application in \cref{sec:dcoi}.
I then speculate on the details of this mapping,
lay out the objectives for such a framework,
and list possible first steps towards accomplishing them in \cref{sec:lattice}.
There is much prior and related work that this project relies on and relates to,
which I divide into work I have personally contributed to (\cref{sec:prior})
and other work in this space (\cref{sec:related}).

\section{Proof assistant extensions in practice} \label{sec:extensions}

To look at extensions in practice,
let us focus on three popular proof assistants:
Rocq~\citep{coq}, Agda~\citep{agda}, and Lean~\citep{lean}.
Broadly speaking, they are all based on variants of MLTT or CIC,
and have dependent functions, type universes, and inductive types,
which are sufficient to encode a wide variety of logical propositions and proofs.

\subsection{Built-in features}

Each of these proof assistants include features that extend the power of their foundations;
below are a few notable extensions,
some of which are hidden behind option flags.

\paragraph{Impredicativity.}
Rocq and Lean, being based on CIC,
feature a universe $\kw{Prop}$ of propositions.
This universe is \emph{impredicative},
meaning that a universal quantification (\ie dependent function type)
$\forall (x : A) \mathpunct{.} B$ is a proposition if $B$ is a proposition,
regardless of the universe in which $A$ lives,
which may be larger than $\kw{Prop}$.
Inductive types may also be defined in $\kw{Prop}$,
which permits its constructors to have argument types
in universes larger than $\kw{Prop}$.
Such inductives are said to be \emph{large}.

Impredicativity allows for self-referential propositions by quantifying over $\kw{Prop}$.
For instance, given a proof that all propositions imply their double negation,
$$\mathit{dn}: \forall (P : \kw{Prop})\mathpunct{.} P \to \neg \neg P,$$
the double negation of this proposition itself holds as well by self-application.
$$\mathit{dn} \; (\forall (P : \kw{Prop})\mathpunct{.} P \to \neg \neg P) \; \mathit{dn} : \neg \neg (\forall (P : \kw{Prop})\mathpunct{.} P \to \neg \neg P)$$
In contrast, in the predicative setting,
a quantification over a universe $\kw{Type}_0 : \kw{Type}_1$ itself has type $\kw{Type}_1$,
and in particular,
$$\Pi (A : \kw{Type}_0)\mathpunct{.} A \to \neg \neg A : \kw{Type}_1,$$
so an element of this type may not be applied to the type itself.

Dually to universal quantification,
we can define existential quantification $\exists (x : A) \mathpunct{.} B$
over types $A$ in larger universes as a large inductive proposition,
\ie a dependent pair type in $\kw{Prop}$.
This enables us to state, for example,
the surjectivity of a function over the naturals as a predicate,
even though the naturals are not a proposition.
%
\begin{align*}
  \mathit{surj} &: (\kw{Nat} \to \kw{Nat}) \to \kw{Prop} \\
  \mathit{surj} &\coloneqq \lambda f \mathpunct{.}
    \forall (y : \kw{Nat}) \mathpunct{.} \exists (x : \kw{Nat}) \mathpunct{.} f \; x \equiv y
\end{align*}

\paragraph{Definitional proof irrelevance.}
A universe of propositions is said to be \emph{strict}
when the inhabitants of its propositions are definitionally equal
(\ie proof irrelevant)
and thus treated as interchangeable during type checking.
Lean's $\kw{Prop}$ is always strict, while
Rocq has a separate $\kw{SProp}$ universe of proof-irrelevant propositions,
and Agda has a predicative hierarchy $\kw{Prop}_i$ of such universes \citep{sprop}.
To use strict $\kw{Prop}$,
Rocq requires the flag \code{Allow StrictProp},
while Agda requires the option \code{\{-\# OPTIONS -{}-prop \#-\}}.

Definitional proof irrelevance is useful to avoid having to prove equalities explicitly.
Consider a relation on two naturals asserting the usual less-than relation,
along with a type of bounded naturals from which the natural contained can be recovered.
%
\begin{align*}
  \cdot \le \cdot &: \kw{Nat} \to \kw{Nat} \to \kw{Prop} \\
  \mathit{BNat} &: \kw{Nat} \to \kw{Type}_0 \\
  \mathit{bNat} &: \Pi (n \; m : \kw{Nat})\mathpunct{.} n \le m \to \mathit{BNat} \; m \\
  \mathit{getNat} &: \Pi (m : \kw{Nat})\mathpunct{.} \mathit{BNat} \; m \to \kw{Nat} \\
  \mathit{getNat} &\; m \; (\mathit{bNat} \; n \; m \; p) \rightsquigarrow n
\end{align*}
%
A desirable property of bounded naturals is that
two bounded naturals are equal if their contained naturals are equal.
%
\begin{align*}
  \mathit{eqBNat} &: \forall (m : \kw{Nat}) \; (b_1 \; b_2 : \kw{BNat} \; m) \mathpunct{.}
    \mathit{getNat} \; m \; b_1 \equiv \mathit{getNat} \; m \; b_2 \to b_1 \equiv b_2
\end{align*}
%
If we try to prove this by destructing $b_1$ and $b_2$ as
$(\mathit{bnat} \; n_1 \; m \; p_1)$ and $(\mathit{bnat} \; n_2 \; m \; p_2)$,
$\mathit{getNat} \; m \; b_1$ and $\mathit{getNat} \; m \; b_2$ reduce to $n_1$ and $n_2$.
%
\begin{align*}
  \mathit{eqBNat} \; m \; (\mathit{bnat} \; n_1 \; m \; p_1) \; (\mathit{bnat} \; n_2 \; m \; p_2)
    &: n_1 \equiv n_2 \to \mathit{bnat} \; n_1 \; m \; p_1 \equiv \mathit{bnat} \; n_2 \; m \; p_2
\end{align*}
%
While we have an equality $n_1 \equiv n_2$,
we do \emph{not} have a proof of $p_1 \equiv p_2$%
\footnote{Technically, this requires a proof of the equality
where $p_1$ has been transported across the equality $n_1 \equiv n_2$
so that it has the same type as $p_2$.}.
Depending on how $\cdot \le \cdot$ is implemented,
it may be possible to prove propositionally that any two inequality proofs are equal.
Alternatively, if propositions are definitionally proof irrelevant,
the inequality proofs can be ignored.
Then rewriting the goal by the given equality is sufficient
for it to be proven by reflexivity.
%
\begin{align*}
  \mathit{eqBNat} \; m \; (\mathit{bnat} \; n_1 \; m \; p_1) \; (\mathit{bnat} \; n_2 \; m \; p_2) \; e \;
    &: \mathit{bnat} \; n_1 \; m \; p_1 \equiv \mathit{bnat} \; n_2 \; m \; p_2 \\
    &\coloneqq \kw{rewrite} \; e \; \kw{in} \; \kw{refl}
\end{align*}

\paragraph{Uniqueness of identity proofs (UIP).}
The \emph{uniqueness of identity proofs} (UIP)
asserts that inhabitants of the same propositional equality
are themselves propositionally equal.
It can be proven using \emph{Axiom K}~\citep{axiomk},
a computational eliminator for propositional equalities of type $a \equiv a$.
%
\begin{align*}
  &K : \forall (A : \kw{Type}) \; (a : A) \; (P : a \equiv a \to \kw{Prop}) \; (p : a \equiv a) \mathpunct{.}
    P \; \kw{refl} \to P \; p \\
  &K \; A \; a \; P \; \kw{refl} \; d \rightsquigarrow d
\end{align*}
%
Agda's default pattern matching behaviour,
which permits matching on an equality of $a \equiv a$ as reflexivity,
admits a proof of UIP as well as defining Axiom K.
While not inherently part of Rocq's type theory,
Axiom K is axiomatized in the standard library as \code{Logic.Eqdep.eq\_rect\_eq}.

UIP is similarly useful to avoid reasoning about equalities between equalities
when the only canonical proof of an equality is reflexivity,
especially in settings without proof irrelevance.
While UIP augments the reasoning power of the type theory,
there are inductive types whose equality proofs are already propositionally equal.
In particular, if a type has decidable equality,
\ie $(x \equiv y) \vee \neg (x \equiv y)$ for any given $x, y$ of that type,
then its equalities are themselves equal~\citep{hedberg}.

\paragraph{Strong elimination.}
Destructing or eliminating an element of an inductive datatype
into a type at a larger universe is known as \emph{strong} or \emph{large} elimination.
That is, a term whose type is in $\kw{Type}_i$
is eliminated to return a term whose type is in $\kw{Type}_j$
for some $j > i$.
For proofs of inductive propositions in $\kw{Prop}$,
this includes eliminating into any non-proposition type.

Strong elimination is a necessary ingredient in discriminating constructors
of proof-relevant datatypes, such as the booleans.
While $\kw{true}$ and $\kw{false}$ are syntactically distinct,
proving their propositional inequivalence requires lifting the booleans
to propositions truthhood $\top$ and falsehood $\bot$.
%
\begin{align*}
  \mathit{lift} &: \kw{Bool} \to \kw{Prop} \\
  \mathit{lift} &\coloneqq \lambda b \mathpunct{.} \kw{if} \; b \; \kw{then} \; \top \; \kw{else} \; \bot
\end{align*}
%
The branching expression is a strong elimination because it returns a type,
or equivalently because its return type is $\kw{Prop}$, a universe.
Letting $\kw{cong} \; f$ be a proof of congruence of $f$ over an equality,
to complete the proof of $\kw{true} \equiv \kw{false} \to \bot$,
the trivial proof of truthhood $\kw{tt}$ is rewritten by the lifted equality $\top \equiv \bot$.
%
\begin{align*}
  \mathit{trueNotFalse} &: \kw{true} \equiv \kw{false} \to \bot \\
  \mathit{trueNotFalse} &\coloneqq \lambda e \mathpunct{.} \kw{rewrite} \; (\kw{cong} \; \textit{lift} \; e) \; \kw{in} \; \kw{tt}
\end{align*}

\subsection{Axioms}

Rocq, Lean, and Agda all have mechanisms for defining axioms or postulates,
which are declarations of constants without definitions.
Although not all axioms are consistent,
there are many well-studied additions commonly used in practice
that are worth considering as extensions in their own right.

\paragraph{Extensional principles.}
Some models of type theory semantically equate things
that are not syntactically (either definitionally or propositionally) equal;
extensional principles adds semantic equalities as propositional equalities.
Examples include \emph{function extensionality},
which equates two functions if they are pointwise equal,
and \emph{propositional extensionality},
which equates two propositions if they are bi\"implicated.
%
\begin{align*}
  \mathit{funext} &: \forall (A : \kw{Type}) \; (B : A \to \kw{Type}) \; (f \; g : \Pi (x : A) \mathpunct{.} B \; x) \mathpunct{.}
    (\forall (x : A) \mathpunct{.} f \; x \equiv g \; x) \to f \equiv g \\
  \mathit{propext} &: \forall (A \; B : \kw{Prop}) \mathpunct{.} (A \to B) \to (B \to A) \to A \equiv B
\end{align*}
%
These axioms are found in the standard libraries of Lean as \code{funext} and \code{propext},
and of Rocq as \code{Logic.{\allowbreak}FunctionalExtensionality.{\allowbreak}functional\_{\allowbreak}extensionality}
and \code{Logic.{\allowbreak}PropExtensionality.{\allowbreak}propositional\_{\allowbreak}extensionality}.
A notable consequence of propositional extensionality is propositional proof irrelevance.

Another example is \emph{univalence},
which asserts an equivalence between propositional equality and equivalence,
\ie given two types $A, B$,
the equivalence $(A \equiv B) \simeq (A \simeq B)$ holds.
This is the core principle underlying Homotopy Type Theory~\citep{hott}.
There are several ways to define equivalence;
the idea is that it captures a propositionally proof-irrelevant isomorphism.
Univalence together with proof irrelevance implies propositional extensionality,
since bi\"implicated propositions are isomorphic by irrelevance,
and univalence gives an equality from the isomorphism.
Univalence alone also implies function extensionality
by a more complex argument~\cite[Chapter 4.9]{hott}.

One application of function and propositional extensionality
is encountered when encoding a function as a relation
whose functionality is proven \apost.
This is a frequent pattern in proof assistants,
as inductive relations often have better ergonomic support
than dependently-typed reasoning over functions.
For example, consider a two-place predicate over
a representation of types $\mathit{Ty}$ and terms $\mathit{Tm}$,
which has the type $\mathit{Ty} \to \mathit{Tm} \to \kw{Prop}$.
If we have trouble defining this predicate recursively due to termination issues
or inductively due to strict positivity issues,
we can instead view it as a function from $\mathit{Ty}$
to a predicate $\mathit{Tm} \to \kw{Prop}$ and try encoding it as a relation:
$$R : \mathit{Ty} \to (\mathit{Tm} \to \kw{Prop}) \to \kw{Prop}.$$
Such a relation could be a \emph{logical relation}~\citep{logrel}
used to model typed lambda calculi,
where a $\mathit{Ty}$ is interpreted as a set of $\mathit{Tm}$s.
Functionality of $R$ demonstrates that $\mathit{Ty}$s have unique interpretations.
To show that $R$ is functional, \ie
$$\forall (A : \mathit{Ty}) (P \; Q : \mathit{Tm} \to \kw{Prop}) \mathpunct{.}
  R \; A \; P \to R \; A \; Q \to P \equiv Q,$$
it suffices to show that
$\forall (a : \mathit{Tm})\mathpunct{.} P \; a \leftrightarrow Q \; a$,
since $\forall (a : \mathit{Tm})\mathpunct{.} P \; a \equiv Q \; a$
follows from propositional extensionality,
and finally $P \equiv Q$ from function extensionality.

The disadvantage of axiomatic equalities is that rewriting by them does not reduce,
which can make reasoning about terms rewritten by such equalities difficult.
There are type theories beyond MLTT and CIC that are designed
so that these principles are instead provable theorems,
such as cubical type theories~\citep{bch,cchm,afh,cartesian}
and Cubical Agda~\citep{cubical-agda} for univalence,
and observational type theory~\citep{ott,ott-now,ttobs,ccobs,cicobs}
for function and propositional extensionality.

\paragraph{Classical principles.}
There are a number of classical axioms that do not hold intuitionistically.
The most common is the principle of \emph{excluded middle} (EM),
which asserts that all propositions are either true (inhabited) or false (uninhabited).
EM is equivalent to several other principles,
including \emph{double negation elimination} (DNE),
$\forall (A : \kw{Prop})\mathpunct{.} \neg \neg A \to A$,
and \emph{Peirce's law},
$\forall (A \; B : \kw{Prop})\mathpunct{.} ((A \to B) \to A) \to A$.
More powerful axioms which imply EM include the axiom of choice
and the (in)definite description operators,
which deal with extracting a concrete piece of data
out of merely knowing that such a piece of data exists without constructing it.

Because a large majority of mathematics is done classically,
many communities mechanizing mathematics freely use classical principles.
The axiom of excluded middle, for instance,
is declared in Rocq as \code{Logic.Classical\_Prop.classic},
and in Lean as \code{em}.
The \code{Logic} subdirectory of Rocq's standard library
contains the classical axioms along with proofs about their properties.
Similarly, Lean's mathematical library mathlib~\citep{mathlib}
contains proofs that rely on classical axioms,
and tactics such as \code{tauto} automatically apply classical reasoning.
% There is work towards constructively integrating classical principles into type theory consistently,
% typically in the form of calculi with control operators~\citep{control,mu,mumu,dl,sr}.

\subsection{Extensions and inconsistencies}

One has to be careful that a chosen set of features and axioms
do not render the type theory logically inconsistent and thus useless for proving.
A number of them are known to be incompatible with one another;
below are a few such combinations.

\begin{itemize}[noitemsep,topsep=0pt]
  \item Strong elimination is inconsistent for large impredicative inductives.
    \citet{strong-pair} show that impredicative dependent pairs with pair projections,
    which correspond to strong elimination, are inconsistent.
    \citet{trees} also demonstrates the inconsistency with an inductive type $U$
    with a single constructor of type
    $\forall (X : \kw{Prop})\mathpunct{.} (X \to U) \to U$.
  \item Strong elimination is also inconsistent for inductive propositions
    when $\kw{Prop}$ is proof irrelevant.
    As seen above, strong elimination suffices to show that $\neg (\kw{true} \equiv \kw{false})$.
    If $\kw{Bool}$ is defined in a proof-irrelevant $\kw{Prop}$,
    $\kw{true} \equiv \kw{false}$ would hold by definition,
    which is a contradiction.
  \item Strong elimination is once again inconsistent for inductive propositions
    in the presence of impredicativity and classical principles such as excluded middle.
    A modern implementation of the construction by \cite{em-irr},
    such as \code{Coq.Logic.Berardi} in Rocq's standard library,
    uses excluded middle to derive propositional proof irrelevance,
    which can be used as above to derive a contradiction.
  \item UIP is inconsistent with univalence.
    Intuitively, univalence produces an equality between two types
    given an equivalence between them,
    and there are types that are equivalent in multiple, provably different ways,
    so there are equalities between them that are provably different,
    thus violating UIP.
    Concretely, $\kw{Bool}$ is equivalent to itself in two different ways,
    either by mapping booleans to themselves or to their negation,
    so there are distinct proofs of $\kw{Bool} \equiv \kw{Bool}$
    \citep[Example 3.1.9]{hott}.
\end{itemize}

\begin{figure}[ht]
\centering
\begin{tikzcd}
  & \mathcolor{red}{\varnothing} & & \\
  \cdot \equiv \cdot : \kw{SProp} \arrow[ru,dashed] & & \mathrm{UA} + \kw{SProp} \arrow[lu,dashed] & \\
  \mathrm{UIP} \arrow[u] & \kw{SProp} \arrow[ru] \arrow[lu] & \mathrm{propext} \arrow[u] & \mathrm{UA} \arrow[lu] \\
  & \kw{Prop} \arrow[u] \arrow[ru] & \mathrm{funext} \arrow[ru] & \\
  & \mathrm{base} \arrow[u] \arrow[luu] \arrow[ru] & &                       
\end{tikzcd}
\caption{A compatibility graph of theories with impredicative $\kw{Prop}$, proof irrelevance ($\kw{SProp}$),
  UIP, univalence (UA), function extensionality (funext), propositional extensionality (propext),
  and (in)compatible combinations.}
\label{fig:lattice}
\end{figure}

\Cref{fig:lattice} illustrates some of these relationships between extensions.
The arrows point from one theory to a strictly more expressive one;
for instance, a theory with propositional extensionality extends the equalities
of a theory with a universe of propositions,
and a theory with univalence can derive function extensionality.
At the top of the graph, the dotted arrows indicate
the incompatibility of a theory that implies UIP with one that contains univalence:
there is no possible encompassing theory.

To prevent inconsistencies, proof assistants hide features behind option flags,
or disallow them entirely.
Rocq, Lean, and Agda generally disallow strong elimination for inductive propositions
in $\kw{Prop}$ and $\kw{SProp}$.
The exceptions are \emph{syntactic subsingletons},
which are inductives that syntactically have at most one inhabitant,
such as $\top$, $\bot$, and conjunction of propositions.
While Rocq's impredicative $\kw{Prop}$ universe is not proof irrelevant,
Rocq still forbids strong elimination even for inductives that are not large
to allow the use of classical principles.
Its compiler flag \code{-impredicative-set} enables impredicativity for $\kw{Set}$
while still allowing strong elimination of small impredicative inductives,
but this flag is not well supported.%
\footnote{\url{https://github.com/coq/coq/issues/9458}}
In Agda, the \code{-{}-with-K} flag enables Axiom K
and the \code{-{}-without-K} flag disables it,
while the features enabled by Cubical Agda using the \code{-{}-cubical} flag
prove univalence as a theorem.
There is also a \code{-{}-safe} flag which disallows,
among other combinations,
having both \code{-{}-with-K} and \code{-{}-cubical}.

Proof assistants also provide some limited ability in tracking the usage of features and axioms.
In Rocq, the axioms and unsafe flags used by a definition
can be listed using the command \code{Print Assumptions},
while in Lean, the axioms can be listed using \code{\#print axioms}.
In Agda, along with checking for inconsistent option combinations,
\code{-{}-safe} also ensures the absence of any \code{postulate}s.
Because option flags can be enabled at module-level granularity,
Agda also has notion of \emph{(co)infective} flags:
an infective flag used in one module
must be used by all modules that depend on that module,
while a coinfective flag used in one module
may only depend on modules that also use that flag.

While users of these proof assistants have been getting along fine
with these compiler tools for tracking features and axioms for the past decade or more,
there is plenty of room of improvement;
I have identified three shortcomings these systems.
%
\begin{itemize}
  \item There is no way of asserting against an extension;
    that is, there is no general mechanism to tell the type checker to fail
    if a particular definition uses some feature or axiom.
    Such an assertion would guarantee that definition safe to be used by others
    that use an incompatible extension.
    For instance, ensuring that a proof does not use Axiom K
    means that it may be used by another proof that does use univalence.
    In this specific case, Agda does have the \code{-{}-without-K} flag for this purpose,
    but few options have a corresponding anti-option.
    Meanwhile, Rocq and Lean's axiom printing mechanism
    does not modify type checking behaviour.
  \item The scope of feature flags is too coarse.
    They range from project-level compiler flags
    down to module-level option flags,
    but even this level of granularity is not the appropriate one:
    modules are intended for organizing definitions by semantic content,
    rather than by the collection of features they happen to all use.
    This prevents reuse of a definition in one module inside another module
    if they happen to have incompatible features,
    even if that particular definition does not depend on any features at all.
  \item Completely disallowing an incompatible extension is unnecessarily restrictive.
    For example, when both \code{-{}-safe} and \code{-{}-with-K} are enabled in Agda,
    nothing that uses \code{-{}-cubical} can be mentioned anywhere.
    However, it should be acceptable to use a theory with UIP
    to state and prove the properties of univalence,
    while never eliminating an equality derived from univalence.
    To generalize, definitions from an incompatible extension
    should be permitted in the type of a term but restricted in the term itself,
    so that we can talk \emph{about} an extension without \emph{using} it.
\end{itemize}

An ideal system for enabling and disabling features and axioms, then,
should track which ones are and aren't used, at least at the definition level,
and distinguish between mentioning and using them.
This suggests that a system for dependency tracking would be a good starting point.
Therefore, the Dependent Calculus of Indistinguishability (DCOI)~\citep{dcoi},
a type system that incorporates dependency tracking and dependent types,
could be a suitable framework for tracking the usage of extensions within type theories.

\section{A primer on DCOI} \label{sec:dcoi}

DCOI is a Pure Type System (PTS)~\citep{pts} augmented with dependency tracking~\cite{dcc}.
DCOI$^\omega$~\citep{dcoi-omega} is a logically consistent instantiation
of DCOI's PTS rules and axioms with a predicative universe hierarchy,
making it suitable as a foundation for theorem proving.
In a typing judgement, dependency tracking appears as annotations
on both the variables in the context,
to indicate how they may be used by the term being typed,
and alongside the type of the term,
to indicate how the term itself may be used.
As an example, consider the following derivable typing judgement for a constant function.
$$A :^\HH \kw{Type} \vdash \lambda x^\LL \; y^\HH \mathpunct{.} x :^\LL
  A^\LL \to A^\HH \to A$$
The concrete levels used here are low ($\LL$) and high ($\HH$) where $\LL < \HH$.
In the context of information flow,
these levels correspond to low- and high-security computations
where low-security computations may not inspect the values of high-security ones.
They can also be thought of in terms of computational irrelevance,
where something marked as computationally irrelevant ($\HH$)
must not play a part in the execution of relevant programs ($\LL$),
and may even be erased away after compilation.

This constant function at low,
which returns its first argument $x$ and ignores its second argument $y$,
must therefore mark $x$ and its type as low to return it,
and marking $y$ and its type as high guarantees that it could not return it.
While the body of the low function cannot return a high argument,
its \emph{type} can depend on a high term,
demonstrated by the high-annotated type $A$ in the context,
which is used in the type of the function.
The intuition is that $A$ does not play a part in the run-time execution of the constant function,
but is otherwise permitted to participate in compile-time type checking.

Though the example only uses two levels $\LL$ and $\HH$,
any instance of DCOI is parametrized over some meet-semilattice,
from which its dependency levels are drawn.
Rather than tracking relevance or information flow,
the goal is to track extensions
using a lattice that associates dependency levels to sets of added extensions.
Before discussing this mapping in \cref{sec:lattice},
I summarize shortly the relevant key aspects and properties of DCOI
that highlight how dependency tracking interacts with terms and typing.

\paragraph{Relative relevance.}
The intuition of computational relevance and irrelevance
is not fixed to the low and high levels,
but is a relative concept between any two ordered dependency levels.
Suppose there is a super-high level $\SS$ such that $\LL < \HH < \SS$.
Then just as a low term may not meaningfully use a high term,
a high term also may not meaningfully use a super-high term.
The following derivable typing judgement demonstrates
how these three levels can interact.
$$P :^\HH \kw{Nat}^\SS \to \kw{Prop}, n :^\SS \kw{Nat} \vdash \lambda p^\LL \mathpunct{.} p :^\LL
  (P \; s^\SS)^\LL \to P \; (s + 1)^\SS$$
In the context, $P$ is a high predicate which takes as argument a super-high natural,
along with a super-high natural $n$.
Once again, the term being typed is a low function,
while higher terms are involved in its type.
Although the function is an identity function,
its domain and codomain types are syntactically different applications of $P$,
but this judgement still holds due to \emph{indistinguishability}.

\paragraph{Indistinguishability.}
In general, if $\ell_1 < \ell_2$, then at observer level $\ell_1$,
the function application $f \; x^{\ell_2}$ must be equal
to $f \; y^{\ell_2}$ regardless of what $x$ and $y$ are.
We say that they are \emph{indistinguishable} at level $\ell_1$.
Indistinguishability plays the r\^ole of definitional equality
when checking that two types are the same.
This is why the above example type checks,
since $P \; s^\SS$ is indistinguishable from $P \; (s + 1)^\SS$ at high,
the level of $P$.
Similarly, calling the low-level constant function above $k$,
$k \; x^\LL \; y^\HH$ is indistinguishable from $k \; x^\LL \; z^\HH$ at low,
which expresses the idea that $k$ is truly constant in its second argument.

DCOI internalizes indistinguishability by indexing
its propositional equality type with an observer level.
In particular, the propositional equality
$k \; x^\LL \; y^\HH \equiv^\LL k \; x^\LL \; z^\HH$
is provable by reflexivity since the two sides are already indistinguishable
at low, the observer level of the equality.

\paragraph{Elimination of higher falsehoods.}
The principle that lower-level terms may not meaningfully depend
on higher-level terms means that,
just as lower-level functions may not return higher-level arguments,
destructors that return lower-level terms may not destruct higher-level terms.
This holds even if the term being destructed contains no inner information
(such as $\top$ or an equality proof),
since reducing the destruction on a constructor requires knowing
whether the term being destructed is a constructor at all.

The sole exception is the eliminator for $\bot$,
since it has no constructors, so there is no information to reveal.
The computational interpretation of having a proof of $\bot$ to eliminate
is that we have reached an impossible dead branch,
so what we do with it never matters since it never executes.
The ability of eliminate higher-level proofs of falsehood into lower-level terms
is useful when the type of a function rules out a particular branch,
and the function needs to be assigned a lower level than its type.

\paragraph{Subsumption and downgrading.}
While lower-level terms cannot inspect higher-level terms,
higher-level terms can inspect lower-level terms.
A lower-level term can also be raised to a higher level by \emph{subsumption}:
if a term is well typed at level $\ell_1$,
then it is also well typed with the same type at a higher level $\ell_2 > \ell_1$.

However, if two terms are indistinguishable by some observer level $\ell_2$,
then they can be indistinguishable by a \emph{lower} observer level $\ell_1$ by \emph{downgrading}.
From a security flow perspective, the higher the observer level,
the more secure values may be observed,
so the more things are distinguishable,
since securer values need to be compared as well instead of being ignored.
Going down an observer level means more things are being hidden away,
so more values appear to be indistinguishable from one another.

\section{Lattices of type theories} \label{sec:lattice}

To track extensions on top of a base type theory,
we begin with a bottom dependency level corresponding to this base.
Each new dependency level above bottom would contain one additional construct
corresponding to a new feature or axiom.
For instance, there could be a K eliminator that type checks
only at a level for UIP and above,
or a built-in excluded middle axiom that type checks
only at a level for classical reasoning and above.
%
\begin{align*}
  \kw{K} \; P \; p \; d :^{\kw{uip}} P \; p &&
  \kw{em} \; A :^{\kw{cl}} A \vee \neg A
\end{align*}
\vspace{-1.5\baselineskip}

Because level annotations are part of contexts and typing judgements,
when a particular definition is safe to use is specified with precision,
guaranteeing that a particular definition never exploits an extension without permission.
A definition that can be typed at the bottom level
would be safe to use at all levels by subsumption,
and guaranteed to never employ, say, classical reasoning.
Indistinguishability reflects this guarantee, as it asserts the property that
uses of values from higher forbidden theories can only be trivial,
such as ignoring the value or passing it around uninspected.

As dependency levels form a meet-semilattice,
any two theories must have a meet,
which corresponds to only the constructs that they both have in common,
and which are therefore safe to use in either theory.
If the join of two theories exist,
then the constructs introduced in either one can be used at the joined level.
Crucially, not all joins exist; a UIP level cannot be joined with a univalence level,
since their co\"existence is contradictory.
The shape of the lattice depends on the compatibilities between theories,
as well as implication order of extensions,
since one theory that encompasses the consequences of another
can be placed above that other theory.
The compatibility graph in \cref{fig:lattice}
is an example of a concrete lattice of theories,
where the arrows point towards the greater theory
and indicate the direction in which definitions can be raised.

The property that the type of a term can itself be well typed at any other level
permits proving propositions about an incompatible theory
without causing an inconsistency.
For example, we can assert the computational behaviour of the K eliminator
even in a theory with univalence.
%
\vspace{-\baselineskip}
\begin{align*}
  \kw{refl} :^{\kw{ua}} \kw{K} \; P \; \kw{refl} \; d \equiv^{\kw{uip}} d
\end{align*}
\vspace{-1.5\baselineskip}

Following the rules of DCOI,
each individual theory must be logically consistent.
If an inconsistency exists at any theory,
by the elimination of higher falsehoods,
the inconsistency propagates to all lower theories, including the bottom theory.
Then by subsumption, the inconsistency at the bottom theory
can be raised to propagate to all higher theories,
and the entire lattice would be inconsistent.
This means that if eliminating falsehoods works exactly as in DCOI,
any theory that features nontermination would not be permitted.

As the goal is to exclude incompatible extensions from a proof assistant,
disallowing logically inconsistent theories is a desirable trait.
Nevertheless, there may be a few ways to modify falsehood elimination to permit them.
One way is to instead disallow eliminating falsehoods to lower levels,
only to the same level.
Another is to take ideas from works from the Trellys project,
such as $\lambda^\theta$~\citep{lambda-theta} and Sep$^3$~\citep{sep3},
and impose a value or termination restriction on falsehoods being eliminated.
If the falsehood in an inconsistent theory is nonterminating or not a value,
then it cannot be eliminated at all,
preventing its propagating to lower theories.

One catch is that a theory whose extension is a new definitional equality
(\ie a new rule for indistinguishability) cannot be contained within its level.
Even if that equality is defined for a given observer level,
it will hold for all lower observer levels by downgrading,
and the extension will be available to all lower theories.
This effect cannot be mitigated using restrictive premises,
as violating downgrading violates many other desirable properties,
including transitivity of definitional equality~\citep{dcoi-omega}.
\note{What \emph{does} this mean for $\kw{SProp}$??}

\subsection{Objectives}

This project should answer the following questions:

\begin{enumerate}
  \item \label[question]{item:extensions}
    What kinds of extensions would fit within this framework?
    Some broad classifications of extensions might be
    ones that add new type universes (\eg $\kw{SProp}$),
    ones that expand the rules for existing constructs
    (\eg impredicativity, strong elimination),
    ones that add new computational constructs with reduction rules
    (\eg Axiom K),
    and ones that add new axiomatic constructs without reduction rules
    (\eg function and propositional extensionality, excluded middle).
  \item \label[question]{item:applications}
    What are useful applications of being able to freely refer to other theories?
    The previous example that referred to the K eliminator, while true,
    is stating a trivial fact that is already provable within the UIP theory.
    Are there meaningful theorems about one theory that cannot be proven in that theory,
    but can be proven in a different yet potentially incompatible theory?
  \item \label[question]{item:model}
    How would a particular lattice of theories be modelled
    to show logical consistency?
    Ideally, the technique used to model a particular lattice
    should be broadly applicable and sufficiently extensible
    to be applied to a different lattice without redoing all the work,
    so that adding more extensions remains sustainable.
\end{enumerate}

To answer these questions, the project would be divided into two portions.
The first is an implementation of a type checker for a specific lattice of type theories.
The lattice should contain a sufficiently diverse set of labels and their orders.
\Cref{fig:lattice} is a good place to start,
as it contains theories in different classifications with different interactions.

To evaluate the viability of such a type checker,
a standard library would be implemented to exercise all levels of the lattice.
The standard libraries of Rocq\footnote{\url{https://coq.inria.fr/distrib/current/stdlib/}},
Agda\footnote{\url{https://agda.github.io/agda-stdlib/master/}},
and Lean\footnote{\url{https://leanprover-community.github.io/mathlib4_docs/}}
are good sources for inspiration,
as many of their files use the features and axioms mentioned in \cref{sec:extensions}.
An implementation would also serve to verify which extensions are indeed invalid
by demonstrating the inconsistencies they yield.

Additionally, writing and checking large proofs would be feasible,
which helps with exploring more complex applications.
One example is computing \emph{semisimplicial types},
which is an open problem in Homotopy Type Theory~\citep{hott}.
Two-level type theory (2LTT)~\citep{2ls,2ltt} is one solution
that contains separate theories with univalence and with UIP,
and Agda implements 2LTT as the \code{-{}-two-level} extension.
We can use libraries\footnote{\eg \url{https://github.com/UnivalencePrinciple/2LTT-Agda}}
for 2LTT in Agda as inspiration for testing whether this project's implementation
of orthogonal univalence and UIP extensions can handle the same proof load.
\Cref{sec:multi} discusses some more details about 2LTT.

The useability of the implementation will inform the design of the system,
especially for considerations beyond DCOI,
such as level inference and level polymorphism.
Annotating definitions and arguments with every single extension it uses
is an unreasonable burden on a practical proof assistant user.
The prototype implementation accompanying DCOI~\citep{dcoi-artifact}
has rudimentary level inference defaulting to a minimum level.
However, the lattice implemented is the usual order of the naturals,
and the examples provided typically use no more than two levels.
An implementation with a lattice containing incompatible extensions
and applications that involve more than two theories
would pinpoint what is required from level inference in practice
for a more sophisticated inference algorithm.

Rewriting libraries in this implementation is also an exercise
in determining where code duplication occurs
and whether level polymorphism would help eliminate it.
Although subsumption allows lifting definitions vertically,
so to speak, from lower theories to higher ones,
it does not allow transporting definitions horizontally
from one theory to a different, incompatible theory.
In addition, while a function can quantify over terms at specific levels,
it cannot quantify over \emph{all} such levels,
or over levels that satisfy some ordering constraint.
Having a lot of examples in the implementation
would reveal whether these are real concerns to be addressed
and what kind of level polymorphism could address them.

% With an implementation, useability concerns can be explored,
% such as level inference.
% Annotating definitions and arguments with every single extension it uses
% is an unreasonable burden on a practical proof assistant user,
% and it may be possible to infer the annotations
% either based on the syntactic constructs used
% or on what set of features are required for successful type checking.

The second portion is a formalized and ideally mechanized proof of consistency.
Because consistency is a semantic property and depends on the strength
of the metatheory used to model the type theory,
the formalization should model a lattice with (at least at first)
only one level above the base theory, the simplest nontrivial lattice.
The focus would be on how to combine two different models of type theory,
not on accommodating as many as possible from the outset.

A sensible starting point would be the mechanization of DCOI$^\omega$~\cite{dcoi-omega},
which proves consistency and normalization of what would be the base theory in the lattice,
and picking a reasonable feature to extend it with.
Candidates include UIP or function extensionality,
as they are expected to already hold in the semantic model.
In this case, the challenge is to prove that the base theory does \emph{not}
prove UIP or function extensionality to demonstrate that
other incompatible extensions could be added to the base theory.

Moreover, the semantic model is a syntactic logical relation
indexed by well-founded universe levels,
which may limit its extensibility;
it cannot be straightforwardly extended to accommodate impredicativity,
nor to accommodate typed definitional equality.
If the PTS rules and axioms of DCOI are instead instantiated to
a single impredicative universe with no universe hierarchy or strong elimination,
we can adapt the logical relation for the Calculus of Constructions by \citet{cc}
to prove consistency and normalization.

A possible alternative is to use \emph{syntactic} modelling~\citep{syntactic},
which would involve a type-preserving translation into another type theory
whose consistency is well established,
guaranteeing consistency of the original system.
While there exist syntactic models of other type theories \citep{sprop,ghosts}
with notions of irrelevance, which is one application of indistinguishability,
a syntactic model of dependency tracking with dependent types is unexplored.

% The process of accomplishing these two portions of the project
% should answer \cref{item:properties},
% either by the implementation revealing unexpected examples that can or cannot be type checked,
% or by metatheoretical properties that hold based on the modelling technique chosen.
% Only once these properties are revealed will we know what further work can be done,
% from augmenting the implementation closer to a practical proof assistant,
% to proving more complex theorems like normalization and decidability of type checking,
% or proving consistency for a larger lattice.

\section{Prior work} \label{sec:prior}

This project builds on prior work on DCOI~\citep{dcoi} and DCOI$^\omega$~\citep{dcoi-omega},
on both of which I am second author.
For the former paper,
I implemented a prototype type checker for DCOI augmented with inductive types
by extending the minimal dependent type checker \texttt{pi-forall}~\citep{pi-forall},
and wrote examples using the type checker and motivating examples for DCOI.
I also proved a few of the lemmas in the mechanization.
For the latter paper,
I wrote about half of the prose, mostly for the earlier sections,
and proved a few of the lemmas as well.
As part of an investigation toward incorporating a relational model for DCOI,
I mechanized a PER model for MLTT based on the logical relation
used to prove consistency of DCOI$^\omega$,
but ultimately the gap between MLTT and DCOI could not be bridged,
so this work does not appear in the final paper.

Outside of DCOI, I have worked on Stratified Type Theory (StraTT)~\citep{stratt},
which annotates typing judgements similarly to dependency tracking,
but the annotations are universe levels.
Instead of stratifying universes into a hierarchy,
typing judgements themselves are stratified,
and there is a single universe whose type is itself.
To ensure consistency in the presence of this \emph{type-in-type} rule,
dependent functions may only quantify over types at strictly lower levels,
which enforces predicativity.
Although StraTT is not a dependency tracking system in the same way DCOI is,
it demonstrates that there may be multiple ways to retain usage information
that enforces desired properties such as consistency or irrelevance.
Even if the particular setup for DCOI turns out not to be suitable for this project,
it may be reasonable to instead explore a more StraTT-like structure.

\section{Related work} \label{sec:related}

\subsection{Multi-system frameworks} \label{sec:multi}

\paragraph{Two-level type theory.}
The most similar work to extension tracking is two-level type theory (2LTT)~\citep{2ls,2ltt}.
It consists of an inner homotopical type theory with univalence
and an outer intensional type theory with UIP,
along with a conversion operation $\mathop{\uparrow} \cdot$ from the inner theory to the outer.
The inner and outer type theories have independent type formers,
including separate inner (path) equality types $\cdot \equiv^\ii \cdot$
and outer (strict) equality types $\cdot \equiv^\oo \cdot$.
Importantly, converting an inner equality does \emph{not} yield the outer equality type;
otherwise, univalence on inner equalities could be converted to univalence on outer equalities,
which would contradict UIP of the outer equality.

These inner and outer levels are different from dependency levels in DCOI,
where all levels share the same type formers,
and a lower equality can be raised to a higher equality by subsumption.
In particular, if a lattice of type theories includes one that supports UIP,
then that level will prove that proofs of the same equality at \emph{all} levels are themselves equal.
Meanwhile, in 2LTT, UIP only holds for proofs the outer equality
and not for converted proofs of the inner equality.

The conversion operator can be thought of as an explicit subsumption,
and how it interacts with the inner and outer equalities
is similar to how DCOI's propositional equality interacts with indistinguishability
at lower and higher levels.
To demonstrate, given two inner terms $x, y$,
the implication $\mathop{\uparrow} x \equiv^\oo \mathop{\uparrow} y \to x \equiv^\ii y$
holds in 2LTT while the converse generally does not.
Similarly, in DCOI, $x \equiv^\HH y \to x \equiv^\LL y$ holds by downgrading
while the converse also generally does not.

\paragraph{The Trellys project.}
Instead of combining multiple type theories,
the Trellys project focussed on combining dependently-typed logical reasoning
with (potentially nonterminating) functional programming.
The main works within the project are $\lambda^\theta$~\citep{lambda-theta},
which classifies typing judgements of a single language into logical and programmatic fragments;
Zombie~\citep{zombie}, an implementation that closely follows $\lambda^\theta$;
Sep$^3$~\citep{sep3}, which syntactically separate proofs from programs;
and Nax~\citep{nax}, which augments dependent types with Mendler-style recursion schemes.

Of these three, DCOI is closest to $\lambda^\theta$,
whose logical and programmatic classifications are similar to DCOI's dependency levels.
Because the logical fragment is subsumed within the programmatic fragment,
the additional features found in the programmatic fragment can be thought of
as an extension of the logical one.
Notable features of the extension include isorecursive types and unrestricted recursion,
allowing for nonterminating programs.
Normalization of the logical fragment is ensured by
only allowing boxed programs to be applied to its functions,
only allowing unboxing of values,
and restricting reduction to call by value.
While the boxing mechanism is similar to domain level annotations in DCOI,
the value restrictions are specific to handling presence of potential divergence.

\Citet{proofs-programs} extends the work done for $\lambda^\theta$
by looking at a number of languages with logical and programmatic fragments,
beginning with a simply typed calculus with classifications (also named $\lambda^\theta$),
extending it with types dependent on terms in LF$^\theta$,
then further extending that with terms dependent on types (\ie type polymorphism)
and types dependent on types (\ie type-level computation) in PCC$^\theta$.
The proofs of normalization for these calculi use partially step-indexed logical relations,
where stepping only occurs in the programmatic fragment.
However, the proof technique does not scale up to handle large elimination or a universe hierarchy,
both of which are present in Zombie.

\paragraph{System DE.}
Similarly to PCC$^\theta$, System DE~\citep{system-de}
is a dependent type system with logical and programmatic fragments,
where the logical fragment is again normalizing.
It extends System DC~\citep{system-dc}
and is designed to be suitable as a core calculus
of the Glasgow Haskell Compiler~\citep{ghc};
as such, it deals with explicit coercion proofs.
In contrast to PCC$^\theta$,
System DE has the type-in-type rule and large elimination in the programmatic fragment.

\subsection{Other proof assistants}

\Cref{sec:extensions} broadly covers a number of optional features
and common axioms in Rocq, Lean, and Agda.
There are many other proof assistants of varying relevance not discussed above.

Idris 2~\citep{idris2} is a dependently typed programming language with partiality.
Definitions can be marked as \code{total}, \code{covering}, or \code{partial};
in principle, totality ensures consistency,
covering ensures type safety while allowing divergence,
and partiality does not ensure either.
Because partiality subsumes covering subsumes totality,
these modifiers can also be thought of as members of a lattice.
Idris 2 is closer to $\lambda^\theta$ than to DCOI,
especially because it is also call by value,
and the partiality modifier is designed so that
diverging terms can appear in types and be reasoned about
while not being reduced during type checking.
Although Idris 2 is based on Quantitative Type Theory~\citep{qtt},
there is no formal description of its core type system that describes all of its features,
especially as it is a rapidly evolving language.

F$^\star$~\citep{fstar} is a proof assistant with dependency tracking for different effects.
Its dependency levels include a \code{Tot} level for total programs,
and a \code{Dv} level above it for potentially diverging programs.
In contrast to $\lambda^\theta$,
the total fragment of F$^\star$ may not refer to the diverging fragment,
so the proof of weak normalization of the total fragment
involves a logical relation that does not consider levels above \code{Tot}.
All the effects in F$^\star$ are implemented as indexed monads,
which get compiled away to the new core calculus TotalF$^\star$~\citep{total-fstar};
divergence aside, effects do not extend the internal type system.

\section{Conclusion}

In this report, I have described a number of common extensions
to proof assistants that alter their core type theory when enabled.
These proof assistants have mechanisms for tracking extensions
and ensuring that incompatible extensions may not be used together.
However, extension tracking is external to the type system,
and internalizing them opens up opportunities
for greater precision and expressivity in specifying extension usage.
Such an internalization of extension tracking is reminiscent of dependency analysis,
and in particular of the Dependent Calculus of Indistinguishability (DCOI),
which tracks usage of terms at different dependency levels.
Although primary applications of DCOI are information flow and irrelevance,
I use it as inspiration for a framework that tracks extensions in the same way,
incorporating multiple type theories in one system.
Much work lies ahead to discover the practical expressivity of such a system,
as well as its logical consistency,
for its viability as a core for a proof assistant with extensions.

\clearpage
\noindent
\bibliography{main}
\end{document}